# Cleaning Data Part IV: PDFs

The next circle of Hell on the Dante's Inferno of Data Journalism is the PDF. Governments everywhere love the PDF and publish all kinds of records in a PDF. The problem is a PDF isn't a data format -- it's a middle finger, saying I've Got Your Accountability Right Here, Pal.

It's so ridiculous that there's a constellation of tools that do nothing more than try to harvest tables out of PDFs. There are online services like [CometDocs](https://www.cometdocs.com/) where you can upload your PDF and point and click your way into an Excel file. There are mobile device apps that take a picture of a table and convert it into a spreadsheet. But one of the best is a tool called [Tabula](https://tabula.technology/). It was build by journalists for journalists.

There is a version of Tabula that will run inside of R -- a library called Tabulizer -- but the truth is I'm having the hardest time installing it on my machine, which leads me to believe that trying to install it across a classroom of various machines would be disastrous. The standalone version works just fine, and it provides a useful way for you to see what's actually going on.

Unfortunately, harvesting tables from PDFs with Tabula is an exercise in getting your hopes up, only to have them dashed. We'll start with an example. First, let's load the tidyverse and janitor.

```{r}
#| output: false
library(tidyverse)
library(janitor)
```


## Easy does it

Tabula works best when tables in PDFs are clearly defined and have nicely-formatted information. Here's a perfect example: [active voters by county in Maryland](https://elections.maryland.gov/press_room/2020_stats/Eligible%20Active%20Voters%20by%20County%20-%20PG20.pdf).

[Download and install Tabula](https://tabula.technology/). Tabula works much the same way as Open Refine does -- it works in the browser by spinning up a small webserver in your computer.

When Tabula opens, you click browse to find the PDF on your computer somewhere, and then click import. After it imports, click autodetect tables. You'll see red boxes appear around what Tabula believes are the tables. You'll see it does a pretty good job at this.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_voters.png"))
```

Now you can hit the green "Preview & Export Extracted Data" button on the top right. You should see something very like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_voters2.png"))
```

You can now export that extracted table to a CSV file using the "Export" button. And then we can read it into R:

```{r}
voters_by_county <- read_csv("data/tabula-Eligible Active Voters by County - PG20.csv")

voters_by_county
```

Boom - we're good to go.

## When it looks good, but needs a little fixing

Here's a slightly more involved PDF. Here's a [PDF of 2020 general election results from Fayette County, Pa.](https://raw.githubusercontent.com/openelections/openelections-sources-pa/master/2020/general/Fayette%20PA%202020%20General%20Summary.pdf).

```{r, echo=FALSE}
knitr::include_graphics(rep("images/fayette_1.png"))
```

Looks like a spreadsheet, right? Save that PDF file to your computer in a place where you'll remember it (like a Downloads folder).

Now let's repeat the steps we did to import the PDF into Tabula and autodetect the tables. It should look like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/fayette_2.png"))
```

This is pretty good, but we don't want the "Times Cast" boxes at the top of each contest, because they have a slightly different layout. Go through the pages (there are 9 of them) and click the "x" to the right of those to clear them.

Now you can hit the green "Preview & Export Extracted Data" button on the top right. Using the "Lattice" method, you should see something very like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/fayette_3.png"))
```

You can now export that extracted table to a CSV file using the "Export" button. And then we can read it into R and clean up the column names:

```{r}
fayette_2020 <- read_csv("data/tabula-Fayette PA 2020 General Summary.csv") %>% clean_names()

fayette_2020
```

## Cleaning up the data in R

The good news is that we have data we don't have to retype. The bad news is, it's hardly in importable shape. We have a few things to fix. All the columns with the number of votes in them have commas, which causes R to think they are <chr> columns, not numbers. Let's fix that by re-importing it and calling `mutate` so that those columns are numeric.

```{r}
fayette_2020 <- read_csv("data/tabula-Fayette PA 2020 General Summary.csv") %>% clean_names()

fayette_2020 <- fayette_2020 %>% mutate(election_day=as.numeric(parse_number(election_day)), absentee=as.numeric(parse_number(absentee)), mail_in=as.numeric(parse_number(mail_in)), provisional=as.numeric(parse_number(provisional)),total=as.numeric(parse_number(total)))

fayette_2020
```

Ok, now we have numbers. Next we'll get rid of the rows where `candidate` is NA or the value of `candidate` is literally "Candidate" and also drop the `x8` blank column. To do the former, we'll use the inverse of the `is.na` function by placing an exclamation point before it (you can read that filter as "where candidate is NOT NA") and do the same to exclude "Candidate" from the matching values. For the latter, we'll use select and the minus sign to drop that column

```{r}
fayette_2020 <- fayette_2020 %>% filter(!is.na(candidate)) %>% filter(candidate != "Candidate") %>% select(-x8)

fayette_2020
```
This still isn't perfect - we don't have the offices these folks are running for - but all things considered, that was pretty easy. Many - most? - electronic PDFs aren't so easy to parse. Sometimes you'll need to open the exported CSV file and clean things up before importing into R. Other times you'll be able to do that cleaning in R itself.

Here's the sad truth: THIS IS PRETTY GOOD. It sure beats typing it out. And since many government processes don't change all that much, you can save the code to process subsequent versions of PDFs.
